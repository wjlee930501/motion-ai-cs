"""
LLM Classification using Claude API

í•™ìŠµ ê²°ê³¼(CSUnderstanding)ë¥¼ ë¶„ë¥˜ì— ë°˜ì˜í•˜ì—¬ needs_reply íŒë‹¨ ì •í™•ë„ ê°œì„ 
"""

import json
import re
from typing import Optional

from shared.config import get_settings
from shared.utils import should_escalate_to_sonnet, should_skip_llm
from shared.database import get_db
from shared.models import CSUnderstanding

settings = get_settings()

# CSUnderstanding ìºì‹œ (ë©”ëª¨ë¦¬)
_cs_understanding_cache = {
    "version": 0,
    "text": None,
    "loaded_at": None
}

# Lazy import anthropic
_client = None


def get_anthropic_client():
    global _client
    if _client is None:
        import anthropic
        _client = anthropic.Anthropic(api_key=settings.anthropic_api_key)
    return _client


def get_cs_understanding_context() -> Optional[str]:
    """
    ìµœì‹  CSUnderstandingì„ ë¡œë“œí•˜ì—¬ ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
    5ë¶„ ìºì‹œ ì ìš©
    """
    import time
    global _cs_understanding_cache

    current_time = time.time()
    cache_ttl = 300  # 5ë¶„

    # ìºì‹œê°€ ìœ íš¨í•˜ë©´ ë°˜í™˜
    if (_cs_understanding_cache["text"] is not None and
        _cs_understanding_cache["loaded_at"] is not None and
        current_time - _cs_understanding_cache["loaded_at"] < cache_ttl):
        return _cs_understanding_cache["text"]

    # DBì—ì„œ ìµœì‹  ì´í•´ ë¡œë“œ
    try:
        db = next(get_db())
        understanding = db.query(CSUnderstanding).order_by(
            CSUnderstanding.version.desc()
        ).first()
        db.close()

        if understanding:
            _cs_understanding_cache["version"] = understanding.version
            _cs_understanding_cache["text"] = understanding.understanding_text
            _cs_understanding_cache["loaded_at"] = current_time
            return understanding.understanding_text
    except Exception as e:
        print(f"[LLM] Failed to load CSUnderstanding: {e}")

    return None




def get_recent_conversation_context(chat_room: str, limit: int = 5) -> list[dict]:
    """
    ìµœê·¼ ë©”ì‹œì§€ ë§¥ë½ì„ ì¡°íšŒí•˜ì—¬ ë¶„ë¥˜ ì •í™•ë„ í–¥ìƒ
    
    Args:
        chat_room: ì±„íŒ…ë°© ì´ë¦„
        limit: ì¡°íšŒí•  ë©”ì‹œì§€ ìˆ˜ (ê¸°ë³¸ 5ê°œ)
    
    Returns:
        ìµœê·¼ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"sender_type": "customer", "text": "..."}, ...]
    """
    try:
        db = next(get_db())
        from shared.models import MessageEvent
        
        recent_events = db.query(MessageEvent).filter(
            MessageEvent.chat_room == chat_room
        ).order_by(
            MessageEvent.received_at.desc()
        ).limit(limit + 1).all()  # +1 because current message might be included
        
        db.close()
        
        if not recent_events or len(recent_events) <= 1:
            return []
        
        # í˜„ì¬ ë©”ì‹œì§€ ì œì™¸í•˜ê³  ì—­ìˆœìœ¼ë¡œ ë°˜í™˜ (ì‹œê°„ìˆœ)
        context = []
        for event in reversed(recent_events[1:]):  # Skip the most recent (current)
            context.append({
                "sender_type": event.sender_type,
                "text": event.text_raw[:100] if event.text_raw else ""
            })
        
        return context
        
    except Exception as e:
        print(f"[LLM] Failed to load conversation context: {e}")
        return []

def build_classification_prompt(base_prompt: str) -> str:
    """
    ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ì— CSUnderstanding ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€
    """
    cs_context = get_cs_understanding_context()

    if not cs_context:
        return base_prompt

    # CSUnderstandingì—ì„œ í•µì‹¬ íŒ¨í„´ ì¶”ì¶œí•˜ì—¬ ì¶”ê°€
    context_addition = f"""

---
[í•™ìŠµëœ CS íŒ¨í„´ - ë¶„ë¥˜ ì‹œ ì°¸ê³ ]
{cs_context[:2000]}
---

ìœ„ í•™ìŠµëœ íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”. íŠ¹íˆ needs_reply íŒë‹¨ ì‹œ:
- í•™ìŠµëœ íŒ¨í„´ì—ì„œ "ë‹µë³€ ë¶ˆí•„ìš”" ë˜ëŠ” "ë‹¨ìˆœ ì‘ë‹µ"ìœ¼ë¡œ ë¶„ë¥˜ëœ ìœ í˜•ì€ needs_reply=false
- í•™ìŠµëœ íŒ¨í„´ì—ì„œ "ë¬¸ì˜", "ìš”ì²­", "ë¶ˆë§Œ"ìœ¼ë¡œ ë¶„ë¥˜ëœ ìœ í˜•ì€ needs_reply=true
"""

    return base_prompt + context_addition


# System prompts
EVENT_CLASSIFICATION_SYSTEM = """ë‹¹ì‹ ì€ ë³‘ì› CS ë©”ì‹œì§€ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

ë¶„ë¥˜ ê¸°ì¤€:
- topic: ë©”ì‹œì§€ì˜ ì£¼ì œ (ì•„ë˜ ëª©ë¡ ì¤‘ ì„ íƒ)
- urgency: ê¸´ê¸‰ë„ (critical/high/medium/low)
- sentiment: ê°ì • (positive/neutral/negative/angry)
- intent: ì˜ë„ (ì•„ë˜ 4ê°€ì§€ ì¤‘ ì„ íƒ)
- needs_reply: ë‹µë³€ì´ í•„ìš”í•œ ë©”ì‹œì§€ì¸ì§€ (true/false)
- summary: í•µì‹¬ ë‚´ìš© 1ì¤„ ìš”ì•½ (20ì ì´ë‚´)
- confidence: ë¶„ë¥˜ í™•ì‹ ë„ (0.0~1.0)

Intent (ì˜ë„) - 14ê°€ì§€ ì¤‘ ì„ íƒ:

[ë‹µë³€ í•„ìš” - needs_reply=true]
- inquiry_status: ìƒíƒœ/ì§„í–‰ í™•ì¸ ë¬¸ì˜ (ì˜ˆ: "ë°œì†¡ëë‚˜ìš”?", "ì²˜ë¦¬ëë‚˜ìš”?", "ì–¸ì œ ë˜ë‚˜ìš”?")
- request_action: ì‘ì—… ìš”ì²­ (ì˜ˆ: "í•´ì£¼ì„¸ìš”", "ë¶€íƒë“œë¦½ë‹ˆë‹¤", "ì§„í–‰í•´ì£¼ì„¸ìš”")
- request_change: ë³€ê²½/ìˆ˜ì • ìš”ì²­ (ì˜ˆ: "ìˆ˜ì •í•´ì£¼ì„¸ìš”", "ë³€ê²½ ë¶€íƒë“œë¦½ë‹ˆë‹¤", "ì·¨ì†Œí•´ì£¼ì„¸ìš”")
- complaint: ë¶ˆë§Œ/í´ë ˆì„ (ì˜ˆ: "ì™œ ì•ˆ ë˜ëŠ” ê±°ì£ ?", "ë¬¸ì œê°€ ìˆì–´ìš”", "ì´ê²Œ ë­ì˜ˆìš”")
- question_how: ë°©ë²•/ì‚¬ìš©ë²• ë¬¸ì˜ (ì˜ˆ: "ì–´ë–»ê²Œ í•´ìš”?", "ë°©ë²•ì´ ë­ì˜ˆìš”?")
- question_when: ì¼ì •/ì‹œê°„ ë¬¸ì˜ (ì˜ˆ: "ì–¸ì œ ê°€ëŠ¥í•´ìš”?", "ì‹œê°„ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")
- follow_up: ì´ì „ ìš”ì²­ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ ì œê³µ (ì˜ˆ: "ì•„ê¹Œ ë§ì”€ë“œë¦° ê±´ ì´ê±°ì˜ˆìš”", "ì¶”ê°€ë¡œ ë³´ë‚´ë“œë ¤ìš”")

[ë‹µë³€ ë¶ˆí•„ìš” - needs_reply=false]
- provide_info: ì •ë³´/ìë£Œ ì œê³µ (ì˜ˆ: "ì‚¬ì§„ ë³´ë‚´ë“œë¦½ë‹ˆë‹¤", "ìë£Œì…ë‹ˆë‹¤", íŒŒì¼ ì „ì†¡)
- acknowledgment: í™•ì¸/ë™ì˜ (ì˜ˆ: "ë„¤", "ì•Œê² ìŠµë‹ˆë‹¤", "í™•ì¸í–ˆìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤")
- greeting: ì¸ì‚¬ (ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”", "ìˆ˜ê³ í•˜ì„¸ìš”")
- internal_discussion: ë³‘ì› ìŠ¤íƒœí”„ë¼ë¦¬ ëŒ€í™” (ì˜ˆ: "ê³¼ì¥ë‹˜ ì´ê±° í™•ì¸í•´ì£¼ì„¸ìš”", "ë‚´ê°€ í• ê²Œ", ìŠ¤íƒœí”„ ê°„ í˜¸ì¹­ ì‚¬ìš©)
- reaction: ë‹¨ìˆœ ë¦¬ì•¡ì…˜ (ì˜ˆ: "ã…ã…", "ã…‹ã…‹", "ğŸ‘", "ã…‡ã…‡", ì´ëª¨ì§€ë§Œ ìˆëŠ” ê²½ìš°)
- confirmation_received: ì§ì› ì•ˆë‚´ ì™„ë£Œ í›„ ê³ ê° í™•ì¸ (ì˜ˆ: ì§ì›ì´ "ë³´ë‚´ë“œë ¸ìŠµë‹ˆë‹¤" í›„ â†’ "ê°ì‚¬í•©ë‹ˆë‹¤!", "ì•Œê² ìŠµë‹ˆë‹¤~")
- other: ìœ„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê¸°íƒ€

Topic ëª©ë¡:
- ë°œì†¡/ì „ì†¡ ë¬¸ì œ
- ì˜ˆì•½ ê´€ë ¨
- ê²°ì œ/ì •ì‚°
- ê¸°ëŠ¥ ë¬¸ì˜
- ì˜¤ë¥˜/ì¥ì• 
- ê³„ì •/ë¡œê·¸ì¸
- ë¦¬ë·° ê´€ë ¨
- ê¸°íƒ€ ë¬¸ì˜
- ì¸ì‚¬/ê°ì‚¬

needs_reply íŒë‹¨ ê¸°ì¤€:
- true: inquiry_status, request_action, request_change, complaint, question_how, question_when, follow_up (ë‹µë³€ í•„ìš”)
- false: provide_info, acknowledgment, greeting, internal_discussion, reaction, confirmation_received, other (ë‹µë³€ ë¶ˆí•„ìš”)
- ë§¥ë½ ê³ ë ¤: ì´ì „ ëŒ€í™” íë¦„ì„ ë³´ê³  íŒë‹¨. íŠ¹íˆ:
  * ê³ ê° ë©”ì‹œì§€ê°€ ì—°ì†ë˜ê³  ìŠ¤íƒœí”„ ê°„ í˜¸ì¹­/ì—…ë¬´ ì§€ì‹œê°€ ìˆìœ¼ë©´ â†’ internal_discussion
  * ì§ì›ì´ ì•ˆë‚´ ì™„ë£Œ í›„ ê³ ê°ì˜ "ê°ì‚¬", "ì•Œê² ìŠµë‹ˆë‹¤" â†’ confirmation_received
  * íŒë‹¨ì´ ì• ë§¤í•˜ë©´ needs_reply=true (ì‘ëŒ€ ëˆ„ë½ ë°©ì§€ ìš°ì„ )

ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥í•©ë‹ˆë‹¤."""


TICKET_SUMMARY_SYSTEM = """ë‹¹ì‹ ì€ ë³‘ì› CS í‹°ì¼“ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ëŒ€í™” ë‚´ì—­ì„ ë¶„ì„í•˜ì—¬ ê´€ë¦¬ìê°€ ë¹ ë¥´ê²Œ ìƒí™©ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹ (JSON):
- summary: í•µì‹¬ ìƒí™© 2~3ì¤„ (bullet point, â€¢ ì‚¬ìš©)
- next_action: ë‹¤ìŒ ì¡°ì¹˜ ê¶Œì¥ 1ì¤„
- overall_urgency: ì „ì²´ ê¸´ê¸‰ë„ (critical/high/medium/low)

ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥í•©ë‹ˆë‹¤."""


def parse_json_response(text: str) -> dict:
    """Parse JSON from LLM response, handling potential markdown code blocks"""
    # Remove markdown code blocks if present
    text = text.strip()
    if text.startswith("```"):
        # Find the end of code block
        lines = text.split("\n")
        json_lines = []
        in_block = False
        for line in lines:
            if line.startswith("```"):
                in_block = not in_block
                continue
            if in_block or not line.startswith("```"):
                json_lines.append(line)
        text = "\n".join(json_lines)

    # Try to parse JSON
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        # Try to find JSON object in text
        match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except json.JSONDecodeError:
                pass
        return {}


def classify_event(
    chat_room: str,
    sender_type: str,
    text: str,
    force_sonnet: bool = False
) -> tuple[dict, str]:
    """
    Classify a single message event.

    Args:
        chat_room: Chat room name
        sender_type: 'staff' or 'customer'
        text: Message text
        force_sonnet: Force use of Sonnet model

    Returns:
        tuple: (classification_result, model_used)
    """
    # Skip simple messages (greetings, acknowledgments - no reply needed)
    if should_skip_llm(text):
        return {
            "topic": "ì¸ì‚¬/ê°ì‚¬",
            "urgency": "low",
            "sentiment": "neutral",
            "intent": "other",
            "needs_reply": False,  # Simple acknowledgments don't need a reply
            "summary": text[:20],
            "confidence": 1.0
        }, "skip"

    # Determine model
    if force_sonnet or should_escalate_to_sonnet(text):
        model = settings.anthropic_model_escalate
    else:
        model = settings.anthropic_model_default

    client = get_anthropic_client()

    # P2: ìµœê·¼ ëŒ€í™” ë§¥ë½ ì¡°íšŒ
    conversation_context = get_recent_conversation_context(chat_room)
    
    context_str = ""
    if conversation_context:
        context_lines = []
        for msg in conversation_context:
            sender_label = "ê³ ê°" if msg["sender_type"] == "customer" else "ì§ì›"
            context_lines.append(f"  [{sender_label}] {msg['text']}")
        context_str = "\nì´ì „ ëŒ€í™” ë§¥ë½ (ìµœê·¼ " + str(len(conversation_context)) + "ê°œ):\n" + "\n".join(context_lines) + "\n"

    user_prompt = f"""ì±„íŒ…ë°©: {chat_room}
ë°œì‹ ì ìœ í˜•: {sender_type}
{context_str}
í˜„ì¬ ë©”ì‹œì§€: {text}

ìœ„ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”. ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬ ì˜ë„ì™€ ê¸´ê¸‰ë„ë¥¼ íŒë‹¨í•˜ì„¸ìš”."""

    # í•™ìŠµëœ CS íŒ¨í„´ì„ í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜
    system_prompt = build_classification_prompt(EVENT_CLASSIFICATION_SYSTEM)

    try:
        response = client.messages.create(
            model=model,
            max_tokens=500,
            system=system_prompt,
            messages=[{"role": "user", "content": user_prompt}]
        )

        result = parse_json_response(response.content[0].text)

        # Validate required fields
        if not all(k in result for k in ["topic", "urgency", "confidence"]):
            # Fallback
            result = {
                "topic": "ê¸°íƒ€ ë¬¸ì˜",
                "urgency": "medium",
                "sentiment": "neutral",
                "intent": "other",
                "needs_reply": True,  # Assume needs reply if classification failed
                "summary": text[:20],
                "confidence": 0.5
            }

        # Ensure needs_reply is set (default to True if not provided by LLM)
        if "needs_reply" not in result:
            result["needs_reply"] = True

        # Re-escalate if low confidence and was Haiku
        if model == settings.anthropic_model_default:
            confidence = result.get("confidence", 1.0)
            if confidence < 0.65:
                return classify_event(chat_room, sender_type, text, force_sonnet=True)

        return result, model

    except Exception as e:
        # Fallback on error
        return {
            "topic": "ê¸°íƒ€ ë¬¸ì˜",
            "urgency": "medium",
            "sentiment": "neutral",
            "intent": "other",
            "needs_reply": True,  # Assume needs reply if error occurred
            "summary": text[:20],
            "confidence": 0.3,
            "error": str(e)
        }, "error"


def summarize_ticket(clinic_key: str, events: list[dict]) -> tuple[dict, str]:
    """
    Generate ticket summary from events.

    Args:
        clinic_key: Clinic identifier
        events: List of event dicts with 'sender_type', 'text', 'time'

    Returns:
        tuple: (summary_result, model_used)
    """
    if not events:
        return {
            "summary": "â€¢ ë©”ì‹œì§€ ì—†ìŒ",
            "next_action": "ì¶”ê°€ ì •ë³´ í•„ìš”",
            "overall_urgency": "low"
        }, "skip"

    client = get_anthropic_client()
    model = settings.anthropic_model_escalate  # Always use Sonnet for summaries

    formatted_events = "\n".join([
        f"[{e.get('time', '')}] {e.get('sender_type', 'unknown')}: {e.get('text', '')}"
        for e in events
    ])

    user_prompt = f"""ì±„íŒ…ë°©: {clinic_key}

ëŒ€í™” ë‚´ì—­:
{formatted_events}

ìœ„ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""

    try:
        response = client.messages.create(
            model=model,
            max_tokens=500,
            system=TICKET_SUMMARY_SYSTEM,
            messages=[{"role": "user", "content": user_prompt}]
        )

        result = parse_json_response(response.content[0].text)

        # Validate required fields
        if not all(k in result for k in ["summary", "next_action", "overall_urgency"]):
            result = {
                "summary": "â€¢ ìš”ì•½ ìƒì„± ì‹¤íŒ¨",
                "next_action": "ìˆ˜ë™ í™•ì¸ í•„ìš”",
                "overall_urgency": "medium"
            }

        return result, model

    except Exception as e:
        return {
            "summary": f"â€¢ ìš”ì•½ ì˜¤ë¥˜: {str(e)[:50]}",
            "next_action": "ìˆ˜ë™ í™•ì¸ í•„ìš”",
            "overall_urgency": "medium",
            "error": str(e)
        }, "error"


# Urgency to priority mapping
URGENCY_TO_PRIORITY = {
    "critical": "urgent",
    "high": "high",
    "medium": "normal",
    "low": "low"
}

PRIORITY_ORDER = ["low", "normal", "high", "urgent"]


def get_priority_from_urgency(urgency: str) -> str:
    """Convert LLM urgency to ticket priority"""
    return URGENCY_TO_PRIORITY.get(urgency, "normal")


def should_upgrade_priority(current: str, new: str) -> bool:
    """Check if new priority is higher than current"""
    try:
        return PRIORITY_ORDER.index(new) > PRIORITY_ORDER.index(current)
    except ValueError:
        return False
